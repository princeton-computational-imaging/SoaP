{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J5E1CJukt0H"
      },
      "source": [
        "### Section 0: Setup\n",
        "If running this in Google Colab, make sure that you are connected to a GPU instance and run the install script below. It should (hopefully) take about 2-5mins to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI4QE-aXkt0J"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Check if GPU exists\n",
        "try:\n",
        "    subprocess.check_output('nvidia-smi')\n",
        "    print(\"GPU is enabled.\")\n",
        "    # Check if running in Google Colab\n",
        "    if 'COLAB_GPU' in os.environ:\n",
        "      # taken from https://colab.research.google.com/github/nerfstudio-project/nerfstudio/blob/main/colab/demo.ipynb\n",
        "      %cd /content/\n",
        "      !pip install --upgrade pip\n",
        "      !pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "      # Installing TinyCuda\n",
        "      %cd /content/\n",
        "      !gdown \"https://drive.google.com/u/1/uc?id=1-7x7qQfB7bIw2zV4Lr6-yhvMpjXC84Q5&confirm=t\"\n",
        "      !pip install tinycudann-1.7-cp310-cp310-linux_x86_64.whl\n",
        "      !pip install commentjson\n",
        "      !pip install pytorch_lightning\n",
        "      # broken cuda version\n",
        "      !pip uninstall -y torchaudio\n",
        "    else:\n",
        "      print(\"COLAB_GPU not detected\")\n",
        "except FileNotFoundError as e:\n",
        "    print(\"GPU is not enabled in this notebook.\")\n",
        "    print(\"Please select 'Runtime -> Change runtime type' and set the hardware accelerator to GPU.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfKFeJKQPPmc"
      },
      "outputs": [],
      "source": [
        "# Clone repo (Colab only)\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    !git clone https://github.com/princeton-computational-imaging/SoaP/\n",
        "    %cd /content/SoaP/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "866qbOL9kt0K"
      },
      "outputs": [],
      "source": [
        "!wget https://soap.cs.princeton.edu/shade_map.npy -P data/\n",
        "!wget https://soap.cs.princeton.edu/demo.zip -P data/\n",
        "!unzip data/demo.zip -d data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB_OzWSbcd83"
      },
      "source": [
        "### Section 1: (Optional) What is a `frame_bundle.npz`?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut2yAwYZc1Ei"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_NBzSevc4pP"
      },
      "source": [
        "Load data from disk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRhSQ01dc6m8"
      },
      "outputs": [],
      "source": [
        "bundle_path = \"data/demo/dragon/compressed_frame_bundle.npz\"\n",
        "# convert to dictionary - important, by default npz load as a namespace which can have odd behaviour\n",
        "bundle = dict(np.load(bundle_path, allow_pickle=True))\n",
        "# remove extra dimensions\n",
        "utils.de_item(bundle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0FAm169dL_d"
      },
      "source": [
        "Our bundle contains four sets of data:  \n",
        "1. `motion` : device motion data including rotation, gravity, and acceleration  \n",
        "2. `raw_[x]` : Bayer RAW frames enumerated from `0` to `num_raw_frames - 1`, with associated metadata  \n",
        "3. `rgb_[x]` : Processed Apple RGB frames enumerated from `0` to `num_rgb_frames - 1`, with associated metadata  \n",
        "4. `depth_[x]` : Apple depth maps enumerated from `0` to `num_depth_frames - 1`, with associated metadata  \n",
        "\n",
        "Lets take a closer look at this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYldYeSxdLCZ"
      },
      "outputs": [],
      "source": [
        "bundle[\"motion\"].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbsVcU-LdQBt"
      },
      "source": [
        "The motion data `motion` contains:  \n",
        "1. `frame_count` : what frame was being recorded when the associated motion data was recorded. There can be multiple motion values for the same frame as the frequency of the accelerometer/gyroscope (100Hz) is higher than the framerate we're recording at (21fps).\n",
        "2. `timestamp` : absolute device time at which measurements were recorded\n",
        "3. `quaternion` : device relative rotation expressed in quaternion format\n",
        "4. `rotation_rate` : velocity of device rotation expressed in roll-pitch-yaw\n",
        "5. `roll_pitch_yaw` : device relative rotation expressed in roll-pitch-yaw\n",
        "6. `acceleration` : device relative acceleration (with gravity removed) expressed in x-y-z\n",
        "7. `gravity` : acceleration due to gravity expressed in x-y-z\n",
        "8. `num_motion_frames` : number of recorded measurements\n",
        "\n",
        "As an example lets plot the device roll over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xxPIJ6IdYZS"
      },
      "outputs": [],
      "source": [
        "roll_pitch_yaw =  bundle[\"motion\"][\"roll_pitch_yaw\"] # [3,N]\n",
        "timestamp =  bundle[\"motion\"][\"timestamp\"] # [N]\n",
        "roll = roll_pitch_yaw[:,0]\n",
        "pitch = roll_pitch_yaw[:,1]\n",
        "yaw = roll_pitch_yaw[:,2]\n",
        "\n",
        "plt.plot(timestamp, roll)\n",
        "plt.ylabel(\"Roll [Rad]\")\n",
        "plt.xlabel(\"Device Time [s]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF1jRJgudb3g"
      },
      "source": [
        "RAW image data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9pcNfyEdaTP"
      },
      "outputs": [],
      "source": [
        "frame = 0 # change this to view other frames\n",
        "raw = bundle[f\"raw_{frame}\"]\n",
        "rgb = bundle[f\"rgb_{frame}\"]\n",
        "depth = bundle[f\"depth_{frame}\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9B2Q6kjdPiK"
      },
      "outputs": [],
      "source": [
        "print(raw.keys())\n",
        "print(\"height:\", raw['height'], \"width:\", raw['width'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x0KZnv9dgDg"
      },
      "source": [
        "Each `raw` frame consists of:\n",
        "1. `frame_count` : frame number, ranges from 0 - `num_raw_frames`\n",
        "2. `timestamp` : absolute device time at which frame was recorded\n",
        "3. `height, width` : frame dimensions (**WARNING**: these may not match the expected orientation of the frame, i.e. if you are recording with the phone vertical or horizontal, the `width` does not change and always refers to the long side of the capture)\n",
        "4. `ISO`, `exposure_time`, `aperture` : camera ISO, exposure time (seconds), and f-stop used to capture the image\n",
        "5. `brightnesss` : the estimated 'brightness' of the scene, honestly not sure what this is (pls message me if you know)\n",
        "6. `shutter_speed` : inverse of `exposure_time`\n",
        "7. `black_level`, `white_level`: min and max real RAW values\n",
        "8. `raw`, 4032 x 3024 single channel, 14-bit mosaiced bayer CFA frame\n",
        "\n",
        "Lets look at the RAW image data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaAoETy-dhrm"
      },
      "outputs": [],
      "source": [
        "raw_img = raw[\"raw\"]\n",
        "\n",
        "# use simple demosaicing the fill gap values (see paper supplemental)\n",
        "raw_demosaiced = utils.raw_to_rgb(torch.tensor(raw_img[None,None].astype(np.int32)))[0].permute(1,2,0)\n",
        "raw_demosaiced = raw_demosaiced/raw_demosaiced.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-yzxCOrdvK6"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "axes[0].imshow(raw_img, cmap=\"gray\")\n",
        "axes[0].set_title(f\"Frame {frame} Mosaiced Raw\")\n",
        "im = axes[1].imshow(raw_demosaiced)\n",
        "axes[1].set_title(f\"Frame {frame} De-Mosaiced Raw\")\n",
        "\n",
        "fig.subplots_adjust(right=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDdX44-4dyM8"
      },
      "source": [
        "If we zoom into a small patch of the above mosaiced RAW we can see the Bayer CFA pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE62TZPGdxxb"
      },
      "outputs": [],
      "source": [
        "plt.imshow(raw_img[:8,:8], cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_s4Xiosd2Op"
      },
      "source": [
        "Applying the shade map to this data we see how it corrects for the vignetting on the edges of the scene:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMMBTfHFkt0L"
      },
      "outputs": [],
      "source": [
        "shade_map = np.load(\"data/shade_map.npy\")\n",
        "raw_img_deshade = raw[\"raw\"] * shade_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4RkBotcd4jn"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "axes[0].imshow(shade_map, cmap=\"gray\")\n",
        "axes[0].set_title(f\"Shade Map\")\n",
        "im = axes[1].imshow(raw_img_deshade, cmap=\"gray\")\n",
        "axes[1].set_title(f\"Frame {frame} Mosaiced Raw + Shade Map\")\n",
        "\n",
        "fig.subplots_adjust(right=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2yFj30BeBTG"
      },
      "source": [
        "Processed RGB and depth data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVzXhZXheEhv"
      },
      "outputs": [],
      "source": [
        "print(rgb.keys())\n",
        "print(\"height:\", rgb['height'], \"width:\", rgb['width'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHDdV9gXeGa0"
      },
      "source": [
        "Each `rgb` frame contains:\n",
        "1. `frame_count`, `timestamp`, `height`, `width` : see `raw` documentation\n",
        "2. `intrinsics`: 3x3 camera intrinsics, see: [documentation](https://developer.apple.com/documentation/avfoundation/avcameracalibrationdata/2881135-intrinsicmatrix)\n",
        "3. `rgb`, 1920 x 1440 3 channel, 8-bit processed RGB frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdU-m4v4eIzo"
      },
      "outputs": [],
      "source": [
        "print(depth.keys())\n",
        "print(\"height:\", depth['height'], \"width:\", depth['width'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMYQNoSueK__"
      },
      "source": [
        "Each `depth` frame contains:\n",
        "1. `frame_count`, `timestamp`, `height`, `width` : see `raw` documentation\n",
        "2. `intrinsic_height`, `intrinsic_width`, `intrinsics` : 3x3 camera intrinsics, with associated frame height and width\n",
        "3. `lens_distortion` : look-up table for radial distortion correction, see: [documentation](https://developer.apple.com/documentation/avfoundation/avcameracalibrationdata/2881129-lensdistortionlookuptable)\n",
        "4. `lens_undistortion` : inverse of `lens_distortion`\n",
        "5. `depth_accuracy` : [accuracy of depth measurements](https://developer.apple.com/documentation/avfoundation/avdepthdata/accuracy), depends on iPhone/iOS version. `1` -> metric depth, `0` -> relative depth\n",
        "6. `depth`, 320 x 240 inverse depth map from monocular cues + LiDAR measurements\n",
        "\n",
        "Here's a preview of what the RGB and iPhone depth data look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Ar-jykeP_a"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 8))\n",
        "axes[0].imshow(rgb['rgb'])\n",
        "axes[0].set_title(\"Frame {0} Image\".format(frame))\n",
        "im = axes[1].imshow(depth['depth'], cmap='RdYlBu')\n",
        "axes[1].set_title(\"Frame {0} iPhone Depth\".format(frame))\n",
        "\n",
        "fig.subplots_adjust(right=0.82)\n",
        "cbar_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])\n",
        "fig.colorbar(im, cax=cbar_ax, label='Depth [m]')\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "print(\"Camera Info at Frame {0}: \\n\".format(frame))\n",
        "print(\"Timestamp:\", rgb['timestamp'], \"\\n\")\n",
        "print(\"Camera Intrinsics: \\n\", rgb['intrinsics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWrA79XSeTJ4"
      },
      "source": [
        "### Section 2: Training on a `frame_bundle.npz`\n",
        "This section will cover how to fit our model to an input RAW frame_bundle.npz, monitor the model's training, and plot its outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg4HzP48et2G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from train import *\n",
        "from utils import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqJAuR2Sf1tt"
      },
      "source": [
        "Lets begin by taking a look at the images in our `compressed_frame_bundle.npz` (this is a sub-sampled `frame_bundle.npz` with 9 images instead of 42 to speed up training/download time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hma153d3fbOI"
      },
      "outputs": [],
      "source": [
        "bundle = dict(np.load(\"data/demo/dragon/compressed_frame_bundle.npz\", allow_pickle=True))\n",
        "utils.de_item(bundle)\n",
        "\n",
        "# plot the first 5 images, downsample 2x for speed\n",
        "fig, ax = plt.subplots(1,5, figsize=(19.5,5))\n",
        "for i in range(5):\n",
        "    ax[i].imshow(bundle[f\"rgb_{i}\"][\"rgb\"][::2,::2])\n",
        "    ax[i].set_title(f\"Image {i}\")\n",
        "\n",
        "# remove ticks\n",
        "for a in ax:\n",
        "    a.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "# adjust spacing\n",
        "plt.subplots_adjust(wspace=0.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N382cN3eyP9"
      },
      "source": [
        "While they barely appear to change, there's actually still more than enough parallax here to recover meaningful depth.  \n",
        "\n",
        "We begin by launching tensorboard so we can see our training progress:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YhuEtN7eyWa"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZyyIi2Bkt0L"
      },
      "source": [
        "Next we run `train.py`. On an RTX 4090 this should train in a couple minutes, on Colab this will be quite a bit slower.  \n",
        "\n",
        "You can refresh the tensorboard window above to watch the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz3HSxV5g7Gy"
      },
      "outputs": [],
      "source": [
        "# only run to 30 epochs to save time, remove the flag to run for default 100 epochs\n",
        "!python3 train.py --name dragon-test --bundle_path data/demo/dragon/compressed_frame_bundle.npz --max_epochs 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EHa5z5djQlI"
      },
      "source": [
        "To view our reconstruction we load the model from disk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mWQko-gjT8w"
      },
      "outputs": [],
      "source": [
        "model = BundleMLP.load_from_checkpoint(\"checkpoints/dragon-test/last.ckpt\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcG0vZwfjW4A"
      },
      "outputs": [],
      "source": [
        "# move model components to GPU\n",
        "model = model.eval()\n",
        "model = model.to('cuda')\n",
        "model.rgb_volume = model.rgb_volume.to('cuda')\n",
        "model.processed_rgb_volume = model.processed_rgb_volume.to('cuda')\n",
        "model.model_rotation = model.model_rotation.to('cuda')\n",
        "model.model_translation = model.model_translation.to('cuda')\n",
        "model.reference_intrinsics = model.reference_intrinsics.to('cuda')\n",
        "model.model_rotation.reference_rotation = model.model_rotation.reference_rotation.to('cuda')\n",
        "\n",
        "# use all encoding levels for inference\n",
        "model.mask = torch.ones_like(model.mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPbUQXv0jaTa"
      },
      "source": [
        "And use `model.generate_outputs` to generate the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9uc5Rnyjgap"
      },
      "outputs": [],
      "source": [
        "rgb, rgb_raw, rgb_processed, depth, depth_img = model.generate_outputs(frame=0, height=1920, width=1440, u_lims=[0.025,0.975], v_lims=[0.025,0.975])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvxRR70Sjksj"
      },
      "source": [
        "Outputs:\n",
        "1. `rgb` : color values I(u,v) output by implicit image model\n",
        "2. `rgb_raw` : corresponding sampled values from bayer RAW volume\n",
        "3. `rgb_processed` : corresponding sampled values from processed RGB volume\n",
        "4. `depth` : depth values D(u,v) from shakes-on-a-plane implicit depth model\n",
        "5. `depth_img` : same as `depth` but with colormap applied for tensorboard visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-uH6IaejnEV"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 8))\n",
        "axes[0].imshow((rgb.permute(1,2,0).cpu()).clip(0,1)) # increase brightness\n",
        "axes[0].set_title(\"Reconstructed Image I(u,v)\")\n",
        "axes[1].imshow(rgb_processed.permute(1,2,0).cpu())\n",
        "axes[1].set_title(\"Processed RGB\")\n",
        "axes[2].imshow(depth.cpu(), cmap=\"RdYlBu\")\n",
        "axes[2].set_title(\"Reconstructed Depth D(u,v)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su3XUolojxfm"
      },
      "source": [
        "### Section 3: Training on PNGs\n",
        "This section is almost identical to the previous one, except we will learn how to convert a stack of `PNGs` into a `frame_bundle.npz` before fitting our model to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ0UL1GYkIaj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import utils.utils as utils\n",
        "from glob import glob\n",
        "from train import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWi3iakjkhvE"
      },
      "source": [
        "You can replace the code below with any filetype (e.g., load an MP4 with OpenCV), as long as `imgs` is a `NxHxWxC` array, where N is the number of frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NosYD8BkU_8"
      },
      "outputs": [],
      "source": [
        "imgs = sorted(glob(\"data/demo/dragon-rgb/*.png\")) # change file extension to match your filetypes\n",
        "imgs = np.array([plt.imread(img)[:,:,:3] for img in imgs]) # remove alpha channel and load\n",
        "\n",
        "print(\"Number of images: \", len(imgs))\n",
        "# plot first image, last image\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
        "ax[0].imshow(imgs[0])\n",
        "ax[0].set_title(\"Image 0\")\n",
        "ax[1].imshow(imgs[-1])\n",
        "ax[1].set_title(f\"Image {len(imgs)-1}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLyPF5h3k8Qe"
      },
      "source": [
        "For our projective camera model to work we'll need to supply it with [camera intrinsics](https://en.wikipedia.org/wiki/Camera_matrix). Here we'll assume we don't have and calibrated intrinsics and will have to create our own.\n",
        "\n",
        "We'll set the camera centers `cx` and `cy` to be the center of the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy72i80WkX5z"
      },
      "outputs": [],
      "source": [
        "cy = imgs.shape[1] // 2 # set centers to the middle of the image\n",
        "cx = imgs.shape[2] // 2\n",
        "print(\"Center y: \", cy, \"\\nCenter x: \", cx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4KSGYJzlZl1"
      },
      "source": [
        "If we don't know the focal length of the camera, we can use a best guess of its FOV (around 70 degrees for a standard phone camera) to calculate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlZVRRrplY_i"
      },
      "outputs": [],
      "source": [
        "focal = min(cx, cy)/np.tan(70 * (np.pi/180/2)) # 70 degree field of view\n",
        "print(\"Focal length (pixels): \", focal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-P3IkZ2lbtF"
      },
      "outputs": [],
      "source": [
        "intrinsics = np.array([[focal, 0, 0],\n",
        "                       [0, focal, 0],\n",
        "                       [cx, cy, 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awjacroSlgeq"
      },
      "source": [
        "These and the images are all we need to make our custom frame bundle, which we save to the same folder as the input data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97_QQvqhle9E"
      },
      "outputs": [],
      "source": [
        "rgb_bundle = {}\n",
        "for i in range(len(imgs)):\n",
        "    rgb = {\"rgb\": imgs[i], \"intrinsics\": intrinsics, \"height\": imgs.shape[2], \"width\": imgs.shape[1]}\n",
        "    rgb_bundle[f'rgb_{i}'] = rgb\n",
        "rgb_bundle['num_rgb_frames'] = len(imgs)\n",
        "rgb_bundle['num_raw_frames'] = 0\n",
        "rgb_bundle['num_depth_frames'] = 0\n",
        "rgb_bundle['motion'] = None\n",
        "np.savez('data/demo/dragon-rgb/frame_bundle.npz', **rgb_bundle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfxU3mMQlpWv"
      },
      "source": [
        "Now we can train our model as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjRx9O-Jlm1K"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv-Ss6PBl6hS"
      },
      "source": [
        "However we now have to add flags `--no_device_rotations`, `--no_phone_depth`, and `--no_raw` to let the training code know that we're only passing in RGB data and nothing else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-RQ4woClzLv"
      },
      "outputs": [],
      "source": [
        "# only run to 30 epochs to save time, remove the flag to run for default 100 epochs\n",
        "!python3 train.py --name dragon-rgb-test --bundle_path data/demo/dragon-rgb/frame_bundle.npz --max_epochs 30 --no_device_rotations --no_phone_depth --no_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv85okr3lzL5"
      },
      "source": [
        "To view our reconstruction we load the model from disk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQCjH4B_lzL5"
      },
      "outputs": [],
      "source": [
        "model = BundleMLP.load_from_checkpoint(\"checkpoints/dragon-rgb-test/last.ckpt\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUr4jWsTlzL5"
      },
      "outputs": [],
      "source": [
        "# move model components to GPU\n",
        "model = model.eval()\n",
        "model = model.to('cuda')\n",
        "model.rgb_volume = model.rgb_volume.to('cuda')\n",
        "model.processed_rgb_volume = model.processed_rgb_volume.to('cuda')\n",
        "model.model_rotation = model.model_rotation.to('cuda')\n",
        "model.model_translation = model.model_translation.to('cuda')\n",
        "model.reference_intrinsics = model.reference_intrinsics.to('cuda')\n",
        "# model.model_rotation.reference_rotation = model.model_rotation.reference_rotation.to('cuda') # doesnt exist\n",
        "\n",
        "# use all encoding levels for inference\n",
        "model.mask = torch.ones_like(model.mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEg6nQvGlzL5"
      },
      "source": [
        "And use `model.generate_outputs` to generate the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_EQO9S1lzL5"
      },
      "outputs": [],
      "source": [
        "rgb, rgb_raw, rgb_processed, depth, depth_img = model.generate_outputs(frame=0, height=1920, width=1440, u_lims=[0.025,0.975], v_lims=[0.025,0.975])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2e610IylzL5"
      },
      "source": [
        "Outputs:\n",
        "1. `rgb` : color values I(u,v) output by implicit image model\n",
        "2. `rgb_raw` : corresponding sampled values from bayer RAW volume\n",
        "3. `rgb_processed` : corresponding sampled values from processed RGB volume\n",
        "4. `depth` : depth values D(u,v) from shakes-on-a-plane implicit depth model\n",
        "5. `depth_img` : same as `depth` but with colormap applied for tensorboard visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdliAs7blzL5"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 8))\n",
        "axes[0].imshow((rgb.permute(1,2,0).cpu()).clip(0,1)) # increase brightness\n",
        "axes[0].set_title(\"Reconstructed Image I(u,v)\")\n",
        "axes[1].imshow(rgb_processed.permute(1,2,0).cpu())\n",
        "axes[1].set_title(\"Processed RGB\")\n",
        "axes[2].imshow(depth.cpu(), cmap=\"RdYlBu\")\n",
        "axes[2].set_title(\"Reconstructed Depth D(u,v)\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4J5E1CJukt0H",
        "GB_OzWSbcd83",
        "GWrA79XSeTJ4",
        "su3XUolojxfm"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
